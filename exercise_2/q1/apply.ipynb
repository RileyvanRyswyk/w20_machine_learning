{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from linclass import linclass\n", "from leastSquares import leastSquares\n", "from plot_ import plot_"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train = {}\n", "test = {}\n", "## Load the data\n", "train.update({'data': np.loadtxt('lc_train_data.dat')})\n", "train.update({'label': np.loadtxt('lc_train_label.dat')})\n", "test.update({'data': np.loadtxt('lc_test_data.dat')})\n", "test.update({'label': np.loadtxt('lc_test_label.dat')})"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Q1 a) Train the classifier using the training dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weight, bias = leastSquares(train['data'], train['label'])"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Q1 b) Evaluate the classifier on the training dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train.update({'prediction': linclass(weight, bias, train['data'])})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print and show the performance of the classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train.update({'acc' : sum(train['prediction'] == train['label'])/len(train['label'])})\n", "print('Accuracy on train set: {0}'.format(train['acc']))\n", "plot_(train['data'], train['label'], weight, bias, 'Train Set')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test the classifier on the test dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test.update({'prediction': linclass(weight, bias, test['data'])})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print and show the performance of the classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test.update({'acc' : sum(test['prediction'] == test['label'])/len(test['label'])})\n", "plot_(test['data'], test['label'], weight, bias, 'Test Set')\n", "print('Accuracy on test set: \\t {0}\\n'.format(test['acc']))"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Bonus: Add outlier to training data, what happens?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Adding outliers...')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train['data'] = np.append(train['data'], [[1.5, -0.4],[1.45, -0.35]], axis = 0)\n", "train['label'] = np.append(train['label'], [[-1],[-1]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train the classifier using the training dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weight, bias = leastSquares(train['data'], train['label'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate the classifier on the training dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train['prediction'] = linclass(weight, bias, train['data'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print and show the performance of the classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train['acc'] = sum(train['prediction'] == train['label'])/len(train['label'])\n", "print('Accuracy on train set: {0}'.format(train['acc']))\n", "plot_(train['data'], train['label'], weight, bias, 'Train Set')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test the classifier on the test dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test['prediction'] = linclass(weight, bias, test['data'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print and show the performance of the classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test['acc'] = sum(test['prediction']==test['label'])/len(test['label'])\n", "print('Accuracy on test set: \\t {0}\\n'.format(test['acc']))\n", "plot_(test['data'], test['label'], weight, bias, 'Test Set')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}